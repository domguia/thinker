{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYSfMRGzmtHi",
        "outputId": "9aba6e8a-dc2a-45c0-8102-083636bf73fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'thinker'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 69 (delta 24), reused 57 (delta 16), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (69/69), 606.24 KiB | 4.63 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/domguia/thinker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzXopkDUmtH5",
        "outputId": "4d0303ae-96af-4f28-db02-9d211d2a6d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/thinker\n"
          ]
        }
      ],
      "source": [
        "%cd thinker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hGnKk0U6LGOh"
      },
      "outputs": [],
      "source": [
        "pip install -q torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHN5to7RmtH8"
      },
      "outputs": [],
      "source": [
        "%run th1nker_run.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhcVabRfO0XC",
        "outputId": "74baa61d-420e-4cfe-f36a-337f4167d8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/thinker\n"
          ]
        }
      ],
      "source": [
        "%cd thinker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MuAFbfOlLBrd"
      },
      "outputs": [],
      "source": [
        "# # number of call step for the model should be evaluated considering task scheme and memory usage\n",
        "# params = dict(\n",
        "#     # data param\n",
        "#     batch_size = (1, 4, 8, 32),\n",
        "#     input_lenght = (16, 64, 128, 256, 512, 1024),\n",
        "#     output_lenght = (16, 64, 128, 256, 512, 1024),\n",
        "\n",
        "#     # model run param\n",
        "#     steps = (1, 4, 8, 16, 32, 64, 128),\n",
        "#     latent = (4, 8, 16, 32, 64, 128),\n",
        "#     memory_context = (16, 32, 64, 128),\n",
        "\n",
        "#     # model weight param\n",
        "#     dim = (32, 64, 128, 256, 512, 1024)\n",
        "#     n_layers = (1,2,3)\n",
        "#     n_heads = 8\n",
        "#     # head_dim = 8\n",
        "#     # hidden_dim = ()\n",
        "# )\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from thinker_model import Th1nker, compute_loss #, CfgNode\n",
        "from numbers_data import NumbersComputeDataset, TASK_SCHEME\n",
        "\n",
        "# should be defined here because of globals()\n",
        "class CfgNode:\n",
        "    \"\"\" a lightweight configuration class inspired by yacs \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "    def merge_from_dict(self, d):\n",
        "        self.__dict__.update(d)\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        self.__dict__.update(**kwargs)\n",
        "        args = [item.strip() for items in args for item in items.split(',')]\n",
        "        self.__dict__.update(**{name: globals()[name] for name in args})\n",
        "    def __str__(self):\n",
        "        return self.__dict__.__str__()\n",
        "\n",
        "cfg = CfgNode(\n",
        "    hdim = 128,\n",
        "    head_size = 16,\n",
        "    number_of_head= 8,\n",
        "    resid_pdrop = 0.1,\n",
        "    attn_pdrop = 0.1,\n",
        "    bias=False,\n",
        "\n",
        "    vocab_size = 270,\n",
        "\n",
        "    input_cache_size = 256,\n",
        "    mem_cache_size = 2048,\n",
        "\n",
        "    min_latent_size = 16,\n",
        "    max_latent_size = 128,\n",
        "    max_output_len = 256,\n",
        "\n",
        "    min_step=2,\n",
        "    max_step=16,\n",
        "\n",
        "    probe_mode=\"number_reg\",\n",
        "    good_pred_loss_treshold=0.5,\n",
        "    decay_coef=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qN0KgWCdvUm",
        "outputId": "aa0a34ce-d7de-4b9d-ff84-35d0d14b7316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning new rate0.001\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = NumbersComputeDataset(TASK_SCHEME)\n",
        "batch_size = 27\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"PyTorch device :\", device)\n",
        "# cfg(vocab_size=NumbersComputeDataset.get_vocabulary_size())\n",
        "model = Th1nker(cfg).to(device)\n",
        "\n",
        "# import torchinfo\n",
        "# torchinfo.summary(model)\n",
        "\n",
        "# Optimizers specified in the torch.optim package\n",
        "learing_rate=0.002\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learing_rate) #, momentum=0.9)\n",
        "\n",
        "loss_tracker = []\n",
        "for idx, (inputs,targets) in enumerate(dataloader):\n",
        "    inputs,targets = inputs.to(device), targets.to(device)\n",
        "    batch_size = inputs.size(0)\n",
        "\n",
        "    logs = CfgNode()\n",
        "    logs('batch_size')\n",
        "\n",
        "    n_step = np.random.randint(cfg.min_step, cfg.max_step)\n",
        "    # m_step = np.random.randint(1, n_step)\n",
        "    logs('n_step')\n",
        "\n",
        "    # #### stop gradient run\n",
        "    # with torch.no_grad():\n",
        "    #     for _ in range(m_step):\n",
        "    #         model.compute_step()\n",
        "    # for _ in range(m_step, n_step-1):\n",
        "    #     model.compute_step()\n",
        "\n",
        "    #### full run with gradient\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    latent_size = np.random.randint(cfg.min_latent_size, cfg.max_latent_size+1)\n",
        "\n",
        "    with torch.device(device):\n",
        "        model.init(batch_size, latent_size)\n",
        "        model.load_input(inputs)\n",
        "    # logs('batch_size, n_step')\n",
        "\n",
        "    losses = []\n",
        "    for i in range(n_step-1):\n",
        "        with torch.device(device):\n",
        "            model.compute_step()\n",
        "        # model.compute_step(with_output=targets.size(1))\n",
        "        # # output = model.compute_step(with_output=y) #causal\n",
        "        # output = model.get_output() #parallel\n",
        "        # loss = compute_loss(output, targets, cfg.probe_mode)\n",
        "        # losses.append(loss)\n",
        "\n",
        "    with torch.device(device):\n",
        "        model.compute_step(with_output=targets.size(1))\n",
        "        output = model.get_output()\n",
        "        loss = compute_loss(output, targets, cfg.probe_mode)\n",
        "\n",
        "    for break_i in range(targets.size(1)-1,-1,-1):\n",
        "        if targets[:,break_i].float().mean() < 20: break\n",
        "\n",
        "    if idx%10==0:\n",
        "        print()\n",
        "        probe, logits, outputs_probe = output\n",
        "        for i in range(targets.size(1)):\n",
        "            val = targets[0,i].item()\n",
        "            print(f\"{val:4d}\", end=', ')\n",
        "            if val == 20: break\n",
        "        print()\n",
        "        for j in range(i+1):\n",
        "            val = outputs_probe[0,j].item()*16\n",
        "            print(f\"{val:.2f}\", end=', ')\n",
        "        print('^')\n",
        "\n",
        "    # losses.append(loss)\n",
        "\n",
        "    # n = len(losses)\n",
        "    # # losses = torch.Tensor(losses)\n",
        "    # # losses = list(map(list, zip(*losses)))\n",
        "    # # losses = [list(filter(lambda x: x, col)) for col in zip(*losses)]\n",
        "    # losses = list(map(lambda x: torch.stack(list(x)).transpose(1,0), zip(*losses)))\n",
        "    # _, probe_loss, pred_loss, _, outputs_probe_losses = losses\n",
        "    \n",
        "    # ## more weight to the good and llast loss\n",
        "    # ## without neglecting the first lower quality\n",
        "    # ## so that the model will value progress in early step\n",
        "    # ## while give more importance to last/good one\n",
        "    # good_ = pred_loss > cfg.good_pred_loss_treshold\n",
        "    # coef_ = good_.clone()\n",
        "    # good_pred_ratio = good_.sum(dim=1)/n\n",
        "    # # coef_[good_] = 0.5/good_.sum(dim=1)\n",
        "    # # coef_[~good_] = 0.5/(n-sum(good_))\n",
        "    # coef_ = torch.where(good_,0.5/good_.sum(dim=1)[:,None],0.5/(n-good_.sum(dim=1)[:,None]))\n",
        "\n",
        "    # ## decay coefficient followed steps\n",
        "    # coef_decay = (cfg.decay_coef*torch.arange(n)/n).softmax(dim=0)\n",
        "    # coef_ = coef_ * coef_decay\n",
        "    \n",
        "    # loss_1 = (probe_loss * coef_).mean()\n",
        "    # loss_2 = (outputs_probe_losses * coef_ * coef_decay).mean()\n",
        "    # loss_3 = (pred_loss * coef_).mean()\n",
        "    # loss = loss_1 + loss_2 + loss_3\n",
        "    \n",
        "    # probe_loss, pred_loss, outputs_probe_losses = probe_loss[:,-1].mean().item(), outputs_probe_losses[:,-1].mean().item(), pred_loss[:,-1].mean().item()\n",
        "    # logs('probe_loss, pred_loss, outputs_probe_losses')\n",
        "    # probe_loss, pred_loss, outputs_probe_losses = probe_loss.mean().item(), outputs_probe_losses.mean().item(), pred_loss.mean().item()\n",
        "\n",
        "\n",
        "    # print(f\"loss {loss:.4f}, good pred : {good_pred_ratio:.4f} = {sum(good_)} / {n} preds over 0.5 treshold\")\n",
        "\n",
        "    _, probe_loss, pred_loss, output_losses, outputs_probe_losses = loss\n",
        "\n",
        "    # loss = probe_loss + pred_loss[:,None] + outputs_probe_losses\n",
        "    # targets < 20\n",
        "    output_loss = output_losses[:,:break_i].mean()\n",
        "    outputs_probe_loss = outputs_probe_losses[:,:break_i].mean()\n",
        "    (output_loss + outputs_probe_loss*16*4).backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if idx%10==0:\n",
        "        print(f\"{idx} :: loss: {outputs_probe_loss.item():.4f} + {output_loss.item():.4f}, n_step: {n_step}, latent_size: {latent_size}\")\n",
        "\n",
        "    # logs('probe_loss, pred_loss, outputs_probe_losses')\n",
        "    # logs('good_pred_ratio,loss')\n",
        "    # print(logs)\n",
        "\n",
        "    loss_tracker.append(outputs_probe_loss.item())\n",
        "\n",
        "    if idx%50==0 and idx>=100:\n",
        "        mean = np.mean(loss_tracker[-50:])\n",
        "        mean_prev = np.mean(loss_tracker[-100:-50])\n",
        "\n",
        "        print(f'averaged loss -> mean_prev:{mean_prev:.4f} mean:{mean:.4f}')\n",
        "        # lr = learing_rate * max(np.abs(mean-mean_prev), 100/idx)\n",
        "        \n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.plot(loss_tracker[10:])\n",
        "        plt.savefig(\"loss.png\")\n",
        "\n",
        "        with open('./train_param.txt','r') as f:\n",
        "            lr = float(f.read())\n",
        "\n",
        "        print('========== learing_rate:', lr)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # if idx%100==0:\n",
        "        #     torch.save(model, \"model.pck\")\n",
        "\n",
        "    # if idx == 1000:\n",
        "    #     lr = learing_rate * 0.1\n",
        "    #     print('learing_rate:', lr)\n",
        "    #     # lr = float(input('Learning new rate: '))\n",
        "    #     for param_group in optimizer.param_groups:\n",
        "    #         param_group['lr'] = lr\n",
        "    # break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
