{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eYSfMRGzmtHi",
        "outputId": "5fcde119-16e9-47d3-a79c-ad32731ef2c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'thinker'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 64 (delta 22), reused 52 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (64/64), 600.81 KiB | 13.65 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/domguia/thinker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IzXopkDUmtH5",
        "outputId": "9bf11421-44a2-4108-bf00-b60a4d02e133",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/thinker\n"
          ]
        }
      ],
      "source": [
        "%cd thinker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q torchinfo"
      ],
      "metadata": {
        "id": "hGnKk0U6LGOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHN5to7RmtH8"
      },
      "outputs": [],
      "source": [
        "%run th1nker_run.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd thinker"
      ],
      "metadata": {
        "id": "VhcVabRfO0XC",
        "outputId": "f5e3b351-9372-4ca7-a032-387a3676194a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/thinker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # number of call step for the model should be evaluated considering task scheme and memory usage\n",
        "# params = dict(\n",
        "#     # data param\n",
        "#     batch_size = (1, 4, 8, 32),\n",
        "#     input_lenght = (16, 64, 128, 256, 512, 1024),\n",
        "#     output_lenght = (16, 64, 128, 256, 512, 1024),\n",
        "\n",
        "#     # model run param\n",
        "#     steps = (1, 4, 8, 16, 32, 64, 128),\n",
        "#     latent = (4, 8, 16, 32, 64, 128),\n",
        "#     memory_context = (16, 32, 64, 128),\n",
        "\n",
        "#     # model weight param\n",
        "#     dim = (32, 64, 128, 256, 512, 1024)\n",
        "#     n_layers = (1,2,3)\n",
        "#     n_heads = 8\n",
        "#     # head_dim = 8\n",
        "#     # hidden_dim = ()\n",
        "# )\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from thinker_model import Th1nker, compute_loss #, CfgNode\n",
        "from numbers_data import NumbersComputeDataset, TASK_SCHEME\n",
        "\n",
        "# should be defined here because of globals()\n",
        "class CfgNode:\n",
        "    \"\"\" a lightweight configuration class inspired by yacs \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "    def merge_from_dict(self, d):\n",
        "        self.__dict__.update(d)\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        self.__dict__.update(**kwargs)\n",
        "        args = [item.strip() for items in args for item in items.split(',')]\n",
        "        self.__dict__.update(**{name: globals()[name] for name in args})\n",
        "    def __str__(self):\n",
        "        return self.__dict__.__str__()\n",
        "\n",
        "cfg = CfgNode(\n",
        "    hdim = 32,\n",
        "    head_size = 8,\n",
        "    number_of_head= 4,\n",
        "    resid_pdrop = 0.1,\n",
        "    attn_pdrop = 0.1,\n",
        "    bias=False,\n",
        "\n",
        "    vocab_size = 270,\n",
        "\n",
        "    input_cache_size = 256,\n",
        "    mem_cache_size = 2048,\n",
        "\n",
        "    min_latent_size = 16,\n",
        "    max_latent_size = 128,\n",
        "    max_output_len = 256,\n",
        "\n",
        "    min_step=2,\n",
        "    max_step=16,\n",
        "\n",
        "    probe_mode=\"number_reg\",\n",
        "    good_pred_loss_treshold=0.5,\n",
        "    decay_coef=4,\n",
        ")"
      ],
      "metadata": {
        "id": "MuAFbfOlLBrd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "metadata": {
        "id": "LnneAz0GPp-x",
        "outputId": "1255bd88-473c-4536-ad38-5d4fbd58d0c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7c381df4ded0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = NumbersComputeDataset(TASK_SCHEME)\n",
        "batch_size = 27\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = \"cpu\"\n",
        "print(\"PyTorch device :\", device)\n",
        "\n",
        "# cfg(vocab_size=NumbersComputeDataset.get_vocabulary_size())\n",
        "model = Th1nker(cfg).to(device)\n",
        "\n",
        "import torchinfo\n",
        "torchinfo.summary(model)\n",
        "\n",
        "# Optimizers specified in the torch.optim package\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "\n",
        "for idx, (inputs,targets) in enumerate(dataloader):\n",
        "    inputs,targets = inputs.to(device), targets.to(device)\n",
        "    batch_size = inputs.size(0)\n",
        "\n",
        "    logs = CfgNode()\n",
        "    logs('batch_size')\n",
        "\n",
        "    n_step = np.random.randint(cfg.min_step, cfg.max_step)\n",
        "    # m_step = np.random.randint(1, n_step)\n",
        "    logs('n_step')\n",
        "\n",
        "    # #### stop gradient run\n",
        "    # with torch.no_grad():\n",
        "    #     for _ in range(m_step):\n",
        "    #         model.compute_step()\n",
        "    # for _ in range(m_step, n_step-1):\n",
        "    #     model.compute_step()\n",
        "\n",
        "    #### full run with gradient\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    latent_size = np.random.randint(cfg.min_latent_size, cfg.max_latent_size+1)\n",
        "\n",
        "    with torch.device(device):\n",
        "        model.init(batch_size, latent_size)\n",
        "        model.load_input(inputs)\n",
        "    logs('batch_size, n_step')\n",
        "\n",
        "    losses = []\n",
        "    for i in range(n_step-1):\n",
        "        with torch.device(device):\n",
        "            model.compute_step()\n",
        "        # model.compute_step(with_output=targets.size(1))\n",
        "        # # output = model.compute_step(with_output=y) #causal\n",
        "        # output = model.get_output() #parallel\n",
        "        # loss = compute_loss(output, targets, cfg.probe_mode)\n",
        "        # losses.append(loss)\n",
        "\n",
        "    with torch.device(device):\n",
        "        model.compute_step(with_output=targets.size(1))\n",
        "        output = model.get_output()\n",
        "        loss = compute_loss(output, targets, cfg.probe_mode)\n",
        "\n",
        "    # losses.append(loss)\n",
        "\n",
        "    # n = len(losses)\n",
        "    # # losses = torch.Tensor(losses)\n",
        "    # # losses = list(map(list, zip(*losses)))\n",
        "    # # losses = [list(filter(lambda x: x, col)) for col in zip(*losses)]\n",
        "    # losses = list(map(lambda x: torch.stack(list(x)).transpose(1,0), zip(*losses)))\n",
        "    # _, probe_loss, pred_loss, _, outputs_probe_losses = losses\n",
        "\n",
        "    # ## more weight to the good and llast loss\n",
        "    # ## without neglecting the first lower quality\n",
        "    # ## so that the model will value progress in early step\n",
        "    # ## while give more importance to last/good one\n",
        "    # good_ = pred_loss > cfg.good_pred_loss_treshold\n",
        "    # coef_ = good_.clone()\n",
        "    # good_pred_ratio = good_.sum(dim=1)/n\n",
        "    # # coef_[good_] = 0.5/good_.sum(dim=1)\n",
        "    # # coef_[~good_] = 0.5/(n-sum(good_))\n",
        "    # coef_ = torch.where(good_,0.5/good_.sum(dim=1)[:,None],0.5/(n-good_.sum(dim=1)[:,None]))\n",
        "\n",
        "    # ## decay coefficient followed steps\n",
        "    # coef_decay = (cfg.decay_coef*torch.arange(n)/n).softmax(dim=0)\n",
        "    # coef_ = coef_ * coef_decay\n",
        "\n",
        "    # loss_1 = (probe_loss * coef_).mean()\n",
        "    # loss_2 = (outputs_probe_losses * coef_ * coef_decay).mean()\n",
        "    # loss_3 = (pred_loss * coef_).mean()\n",
        "    # loss = loss_1 + loss_2 + loss_3\n",
        "\n",
        "    # probe_loss, pred_loss, outputs_probe_losses = probe_loss[:,-1].mean().item(), outputs_probe_losses[:,-1].mean().item(), pred_loss[:,-1].mean().item()\n",
        "    # logs('probe_loss, pred_loss, outputs_probe_losses')\n",
        "    # probe_loss, pred_loss, outputs_probe_losses = probe_loss.mean().item(), outputs_probe_losses.mean().item(), pred_loss.mean().item()\n",
        "\n",
        "\n",
        "    # print(f\"loss {loss:.4f}, good pred : {good_pred_ratio:.4f} = {sum(good_)} / {n} preds over 0.5 treshold\")\n",
        "\n",
        "    _, probe_loss, pred_loss, output_losses, outputs_probe_losses = loss\n",
        "\n",
        "    # loss = probe_loss + pred_loss[:,None] + outputs_probe_losses\n",
        "    output_loss = output_losses.mean()\n",
        "    outputs_probe_loss = outputs_probe_losses.mean()\n",
        "    (output_loss + outputs_probe_loss*16).backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"{idx} loss: {output_loss.item():.4f} + {outputs_probe_loss.item():.4f}, n_step: {n_step}, latent_size: {latent_size}\")\n",
        "\n",
        "    # logs('probe_loss, pred_loss, outputs_probe_losses')\n",
        "    # logs('good_pred_ratio,loss')\n",
        "    # print(logs)\n",
        "\n",
        "    # break\n"
      ],
      "metadata": {
        "id": "e8m_CpMCLdhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.k_cache.device"
      ],
      "metadata": {
        "id": "IM8x-O0XMCRQ",
        "outputId": "77ace427-983e-435e-9978-249137e9cb4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1+1 if 0 else 0"
      ],
      "metadata": {
        "id": "ChOGLDDFW2xc",
        "outputId": "62d68146-faa1-47bc-fec6-523ec6eaba70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}